{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>structure</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34015</td>\n",
       "      <td>this ad is disgusting in so many ways. the vid...</td>\n",
       "      <td>promotes bullying of kids by other kids suppor...</td>\n",
       "      <td>but at a time when schools and parents are bei...</td>\n",
       "      <td>claims:promotes bullying of kids by other kids...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37654</td>\n",
       "      <td>in a press conference on thursday president tr...</td>\n",
       "      <td>the news would actually be more interesting to...</td>\n",
       "      <td>a segment after the press conference cnns jake...</td>\n",
       "      <td>claims:the news would actually be more interes...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3864</td>\n",
       "      <td>australia to hunt down anti vax nurses and pro...</td>\n",
       "      <td>australia to hunt down anti vax nurses and pro...</td>\n",
       "      <td>countries like australia demonize other nation...</td>\n",
       "      <td>claims:australia to hunt down anti vax nurses ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>fake_real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44212</td>\n",
       "      <td>our weekly documentary screening curated by th...</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>our weekly documentary screening curated by th...</td>\n",
       "      <td>claims:&lt;unk&gt;evidences:our weekly documentary s...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1686</td>\n",
       "      <td>says most of perrys chiefs of staff have been ...</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>claims:&lt;unk&gt;evidences:&lt;unk&gt;</td>\n",
       "      <td>REAL</td>\n",
       "      <td>liar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text  \\\n",
       "0  34015  this ad is disgusting in so many ways. the vid...   \n",
       "1  37654  in a press conference on thursday president tr...   \n",
       "2   3864  australia to hunt down anti vax nurses and pro...   \n",
       "3  44212  our weekly documentary screening curated by th...   \n",
       "4   1686  says most of perrys chiefs of staff have been ...   \n",
       "\n",
       "                                               claim  \\\n",
       "0  promotes bullying of kids by other kids suppor...   \n",
       "1  the news would actually be more interesting to...   \n",
       "2  australia to hunt down anti vax nurses and pro...   \n",
       "3                                              <unk>   \n",
       "4                                              <unk>   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  but at a time when schools and parents are bei...   \n",
       "1  a segment after the press conference cnns jake...   \n",
       "2  countries like australia demonize other nation...   \n",
       "3  our weekly documentary screening curated by th...   \n",
       "4                                              <unk>   \n",
       "\n",
       "                                           structure label    dataset  \n",
       "0  claims:promotes bullying of kids by other kids...  FAKE     kaggle  \n",
       "1  claims:the news would actually be more interes...  FAKE     kaggle  \n",
       "2  claims:australia to hunt down anti vax nurses ...  FAKE  fake_real  \n",
       "3  claims:<unk>evidences:our weekly documentary s...  FAKE     kaggle  \n",
       "4                        claims:<unk>evidences:<unk>  REAL       liar  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fin_train = pd.read_csv(\"pipeline/argumentation-based/data/train.csv\")\n",
    "fin_val = pd.read_csv(\"pipeline/argumentation-based/data/validation.csv\")\n",
    "fin_test = pd.read_csv(\"pipeline/argumentation-based/data/test.csv\")\n",
    "\n",
    "df = pd.concat([fin_train, fin_val, fin_test])\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_real\n",
      "1483\n",
      "kaggle\n",
      "3676\n",
      "liar\n",
      "2003\n"
     ]
    }
   ],
   "source": [
    "grouped = df.groupby('dataset')\n",
    "for i in grouped:\n",
    "    print(i[0])\n",
    "    print(len(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_label(df, label):\n",
    "    \"\"\" Returns number of examples with a specific label \"\"\"\n",
    "    return len(df[df[\"label\"] == label])\n",
    "\n",
    "def plot_stats(df):\n",
    "    result = pd.Series(\n",
    "        data={\n",
    "        \"Total\": df.shape[0], \n",
    "        \"# Fake\": n_label(df, \"FAKE\"),\n",
    "        \"# Real\": n_label(df, \"REAL\"),\n",
    "        \"% Fake\": \"{:.2f}\".format(n_label(df, \"FAKE\") / len(df))\n",
    "        }\n",
    "    )\n",
    "    display(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rejoin_data(original, \n",
    "#                 structure) -> dict[pd.DataFrame]:\n",
    "\n",
    "#     merged_df = pd.concat([original.reset_index(drop=False), structure.reset_index(drop=True)], axis=1).dropna()\n",
    "#     # merged_df = merged_df.loc[(structure['claim'] != \"[]\") | (structure['evidence'] != \"[]\")]\n",
    "#     merged_df = merged_df.set_index('ID')\n",
    "\n",
    "#     merged_df['claim'] = merged_df[\"claim\"].apply(lambda x: list(eval(x)))\n",
    "#     merged_df[\"evidence\"] = merged_df[\"evidence\"].apply(lambda x: list(eval(x)))\n",
    "\n",
    "#     # #Merge claim & evidence together as \"structure\"\n",
    "#     merged_df[\"claim\"] = merged_df[\"claim\"].apply(lambda x: ', '.join(x) if x else '<unk>')\n",
    "#     merged_df[\"evidence\"] = merged_df[\"evidence\"].apply(lambda x: ', '.join(x) if x else '<unk>')\n",
    "#     claims = \"claims:\" + merged_df[\"claim\"].values\n",
    "#     evidences = \"evidences:\" + merged_df[\"evidence\"].values\n",
    "\n",
    "#     merged_df[\"structure\"] = claims + evidences\n",
    "#     # merged_df.dropna(inplace=True)\n",
    "#     return merged_df\n",
    "\n",
    "# # Path to batches, without \n",
    "# path = \"pipeline/argumentation-based/argumentation structure/argumentation structure fin/margot\"\n",
    "\n",
    "# path_to_clean_data = \"/Volumes/CentrumZijnsorientatie/Thesis/fake_news/pipeline/argumentation-based/data\"\n",
    "\n",
    "\n",
    "# structure = pd.read_csv(f\"{path}/kaggle/validation_batch_100.csv\", index_col='ID')\n",
    "# original = pd.read_csv(f\"{path_to_clean_data}/kaggle/validation.csv\", index_col='ID')\n",
    "# rejoined = rejoin_data(original, structure)\n",
    "# rejoined.to_csv(f\"{path}/kaggle/validation.csv\", index_label=\"ID\", columns=['text', 'claim', 'evidence', 'structure', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = {}\n",
    "    for f in os.listdir(path):\n",
    "        name = f.split('.')[0]\n",
    "        df = pd.read_csv(f\"{path}/{f}\", usecols=['ID', 'text', 'claim', 'evidence', 'structure', 'label'])\n",
    "        df = df.reset_index(drop=True).set_index('ID')\n",
    "        data[name] = df\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_0 = load_data(\"pipeline/argumentation-based/argumentation structure/argumentation structure fin/margot\")\n",
    "\n",
    "display(annotated_0['fake_real'].shape) \n",
    "display(annotated_0['kaggle'].shape)\n",
    "display(annotated_0['liar'].shape)\n",
    "\n",
    "# TO DO UNNEST, only train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_1 = load_data(\"pipeline/argumentation-based/argumentation structure/argumentation structure fin/margot_1000\")\n",
    "\n",
    "display(annotated_1['fake_real'].shape) \n",
    "display(annotated_1['kaggle'].shape)\n",
    "display(annotated_1['liar'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in annotated_0:\n",
    "    merged = pd.concat([annotated_0[i], annotated_1[i]])\n",
    "    print(\"# Before removing duplicates and adding col\")\n",
    "    print(merged.shape) \n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    merged['dataset'] = i\n",
    "    res[i] = merged\n",
    "    print(merged.shape)\n",
    "    display(merged.head(5))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([res['fake_real'], res['kaggle'], res['liar']])\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(df_final, test_size=0.2)\n",
    "validation, test = train_test_split(validation, test_size=0.25)\n",
    "\n",
    "display(train.shape, validation.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(train)\n",
    "plot_stats(validation)\n",
    "plot_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('pipeline/argumentation-based/argumentation structure/FINAL/train.csv', index='ID')\n",
    "validation.to_csv('pipeline/argumentation-based/argumentation structure/FINAL/validation.csv', index='ID')\n",
    "test.to_csv('pipeline/argumentation-based/argumentation structure/FINAL/test.csv', index='ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_batches(batches, name, path) -> None:\n",
    "    sorted_files = sorted(batches, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "\n",
    "    idx, batched = [], []\n",
    "    for f in sorted_files: \n",
    "        idx.append(int(re.findall(r'\\d+', f)[0]))\n",
    "        batched.append(pd.read_csv(f\"{PATH}/{f}\"))\n",
    "\n",
    "    res = pd.concat(batched, ignore_index=True)\n",
    "    res.set_index(\"ID\", inplace=True)\n",
    "\n",
    "    res.to_csv(f\"{path}/{name}_batch_{idx[0]}-{idx[-1]}.csv\")\n",
    "\n",
    "    for f in sorted_files:\n",
    "        os.remove(f\"{path}/{f}\")\n",
    "\n",
    "PATH = \"pipeline/argumentation-based/argumentation structure/argumentation structure fin/dolly/liar\"\n",
    "\n",
    "# batches_train, batches_test, batches_val = [], [], []\n",
    "\n",
    "# for f in os.listdir(PATH):\n",
    "#     if f.startswith(\"train_\") and not re.match(r'\\w+_\\w+_\\d+-\\d+', f):\n",
    "#         batches_train.append(f)\n",
    "#     if f.startswith(\"validation_\") and not re.match(r'\\w+_\\w+_\\d+-\\d+', f):\n",
    "#         batches_val.append(f)\n",
    "#     if f.startswith(\"test_\") and not re.match(r'\\w+_\\w+_\\d+-\\d+', f):\n",
    "#         batches_test.append(f)\n",
    "\n",
    "\n",
    "# if len(batches_train) > 1:\n",
    "#     combine_batches(batches_train, \"train\", PATH)\n",
    "\n",
    "# if len(batches_test) > 1:\n",
    "#     combine_batches(batches_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pipeline/argumentation-based/argumentation structure/argumentation structure fin/dolly\"\n",
    "\n",
    "res = load_data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    df = res[i]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df['dataset'] = i\n",
    "    df['label'] = df['label'].apply(lambda x: 'FAKE' if x == 0 else ('REAL' if x == 1 else x))\n",
    "    res[i] = df\n",
    "    plot_stats(df)\n",
    "    display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([res['fake_real'], res['kaggle'], res['liar']])\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(df_final, test_size=0.2)\n",
    "validation, test = train_test_split(validation, test_size=0.25)\n",
    "\n",
    "display(train.shape, validation.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('pipeline/argumentation-based/argumentation structure/FINAL/dolly/train.csv', index='ID')\n",
    "validation.to_csv('pipeline/argumentation-based/argumentation structure/FINAL/dolly/validation.csv', index='ID')\n",
    "test.to_csv('pipeline/argumentation-based/argumentation structure/FINAL/dolly/test.csv', index='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Volumes/CentrumZijnsorientatie/Thesis/fake_news/pipeline/argumentation-based/data/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(path_to_clean):\n\u001b[1;32m      7\u001b[0m     tmp \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath_to_clean\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00md\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m     10\u001b[0m         name \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m         \u001b[39m# for t in os.listdir(f\"{path_to_clean}/{d}/{f}\"):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m             \u001b[39m# print(t)\u001b[39;00m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Volumes/CentrumZijnsorientatie/Thesis/fake_news/pipeline/argumentation-based/data/.DS_Store'"
     ]
    }
   ],
   "source": [
    "DATASET = \"fake_real\"\n",
    "path_to_clean = f\"/Volumes/CentrumZijnsorientatie/Thesis/fake_news/pipeline/argumentation-based/data\"\n",
    "\n",
    "res = {}\n",
    "\n",
    "for d in os.listdir(path_to_clean):\n",
    "    tmp = {}\n",
    "\n",
    "    for f in os.listdir(f\"{path_to_clean}/{d}\"):\n",
    "        name = f.split('.')[0]\n",
    "        # for t in os.listdir(f\"{path_to_clean}/{d}/{f}\"):\n",
    "            # print(t)\n",
    "\n",
    "        df = pd.read_csv(f\"{path_to_clean}/{d}/{f}\", index_col=\"ID\")\n",
    "        # index = df_final[df_final['dataset'] == d]\n",
    "        tmp[name] = df\n",
    "\n",
    "    res[d] = tmp\n",
    "\n",
    "# display(res['kaggle']['train'])\n",
    "fr = pd.concat([res['fake_real']['train'], res['fake_real']['validation'], res['fake_real']['test']])\n",
    "fr = fr.loc[df_final[df_final['dataset'] == 'fake_real'].index, :] \n",
    "\n",
    "\n",
    "kaggle = pd.concat([res['kaggle']['train'], res['kaggle']['validation'], res['kaggle']['test']])\n",
    "kaggle = kaggle.loc[df_final[df_final['dataset'] == 'kaggle'].index, :] \n",
    "\n",
    "liar = pd.concat([res['liar']['train'], res['liar']['validation'], res['liar']['test']])\n",
    "liar = liar.loc[df_final[df_final['dataset'] == 'liar'].index, :] \n",
    "\n",
    "df = pd.concat([fr, kaggle, liar])\n",
    "# df.drop_duplicates(inplace=True, keep='first')\n",
    "# print(df.shape)\n",
    "# display(df)\n",
    "# df = df.loc[df_final.index, :]  # works\n",
    "# df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "# for i in res:\n",
    "#     for j in res[i]:\n",
    "        \n",
    "# print(full_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"pipeline/argumentation-based/argumentation structure/FINAL/clean.csv\", index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(df, test_size=0.2)\n",
    "validation, test = train_test_split(validation, test_size=0.25)\n",
    "\n",
    "display(train.shape, validation.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mpipeline/argumentation-based/data/train.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m validation\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mpipeline/argumentation-based/data/validation.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mpipeline/argumentation-based/data/test.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.to_csv('pipeline/argumentation-based/data/train.csv', index='ID')\n",
    "validation.to_csv('pipeline/argumentation-based/data/validation.csv', index='ID')\n",
    "test.to_csv('pipeline/argumentation-based/data/test.csv', index='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
