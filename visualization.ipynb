{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "File contains code to visualize the results of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import evaluate\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for charts, tables\n",
    "model_labels = {'electra-base-discriminator': 'ELECTRA', 'google': 'ELECTRA', 'google/electra-base-discriminator': 'ELECTRA', 'roberta-base': 'RoBERTa', 'bert-base-uncased': 'BERT', 'distilbert-base-uncased': 'DistilBERT'}\n",
    "columns = ['Accuracy', 'Precision', 'Recall', 'F1']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization **manual evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations Dolly\n",
    "dolly = pd.read_csv(\"dolly/data/train.csv\", index_col='ID').reset_index()\n",
    "\n",
    "# Annotations MARGOT\n",
    "margot = pd.read_csv(\"margot/data/train.csv\", index_col='ID').reset_index()\n",
    "\n",
    "df_annotations = pd.merge(dolly, margot, how='inner', left_on='text', right_on='text', suffixes=('_dolly', '_margot')).set_index('ID_dolly')\n",
    "\n",
    "# Only keep columns we need\n",
    "df_annotations = df_annotations[['text', 'claim_margot', 'evidence_margot', 'claim_dolly', 'evidence_dolly', 'dataset_dolly', 'label_dolly']]\n",
    "df_annotations.index.rename('ID', inplace=True)\n",
    "\n",
    "# Remove options of <unk> (MARGOT)\n",
    "df_annotations = df_annotations[df_annotations['claim_margot'] != '<unk>']\n",
    "final_df_annotations = df_annotations[df_annotations['evidence_margot'] != '<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 random texts\n",
    "sample = final_df_annotations.sample(10, random_state=42)\n",
    "\n",
    "# Write out the preprocessed arugmentation-based for manual evaluation\n",
    "sample.to_excel('manual.xlsx', index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manual extractions\n",
    "manual = pd.read_excel(\"manual_evaluation.xlsx\", index_col='ID')\n",
    "display(manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually count the alignments\n",
    "matrix_margot = np.array([\n",
    "    [8, 1],\n",
    "    [2, 9]\n",
    "])\n",
    "\n",
    "# Results Dolly\n",
    "matrix_dolly = np.array([\n",
    "    [8, 2],\n",
    "    [1, 8]\n",
    "])\n",
    "\n",
    "matrix_dolly = matrix_dolly / 10 * 100\n",
    "matrix_margot = matrix_margot / 10 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = ['Claim', 'Evidence']\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "ax = sns.heatmap(matrix_margot, \n",
    "            annot=False,\n",
    "            cmap=\"Reds\",\n",
    "            cbar=False)\n",
    "\n",
    "ax.set_xticklabels(labels, fontsize=8)\n",
    "ax.xaxis.tick_top()\n",
    "ax.yaxis.tick_left()\n",
    "ax.set_yticklabels(labels, fontsize=8)\n",
    "\n",
    "\n",
    "text_colors = [['white' if (i == 0 and j == 0) or (i == len(labels) - 1 and j == len(labels) - 1) else 'black'\n",
    "                for j in range(len(labels))] for i in range(len(labels))]\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        lab = f'MARGOT: {matrix_margot[i, j]}%\\nDolly 2.0: {matrix_dolly[i, j]}%'\n",
    "        ax.text(j + 0.5, i + 0.5, lab,ha='center', va='center', color=text_colors[i][j], fontsize=7)\n",
    "\n",
    "ax.set_xlabel(\"Manual extraction\", fontsize=10)\n",
    "ax.set_ylabel(\"Argument component extraction\", fontsize=10)\n",
    "ax.xaxis.set_label_position('top') \n",
    "ax.xaxis.labelpad = 7\n",
    "ax.yaxis.labelpad = 7\n",
    "\n",
    "fig.add_axes(ax)\n",
    "\n",
    "fig.savefig('confusion.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization on **test** set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline performance\n",
    "with open(\"baseline/performance.json\") as json_file:\n",
    "    baseline_perf = json.load(json_file)\n",
    "\n",
    "# Convert JSON to a table\n",
    "idx = [model_labels[model] for model in baseline_perf]\n",
    "values = [[\"{:.2f}\".format(baseline_perf[model][metric]) for metric in baseline_perf[model]] for model in baseline_perf]\n",
    "df_baseline = pd.DataFrame(values, index=idx, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MARGOT\n",
    "performance_margot = {}\n",
    "for component_name in os.listdir(\"margot/results\"):\n",
    "    path_to_json = f\"margot/results/{component_name}/json\"\n",
    "    for f in os.listdir(path_to_json):\n",
    "        path = os.path.join(path_to_json, f)\n",
    "        with open(path) as f:\n",
    "            json_results = json.load(f)\n",
    "            performance_margot[component_name] = json_results\n",
    "\n",
    "# Convert JSON to table\n",
    "cols, values = [], []\n",
    "for component in performance_margot:\n",
    "    tmp_col, idx = [], []\n",
    "    for model, perf in performance_margot[component].items():\n",
    "\n",
    "        idx.append(model_labels[model])\n",
    "\n",
    "        metric = [(component, metric) for metric in performance_margot[component][model]]\n",
    "        value = [(performance_margot[component][model][metric]) for metric in performance_margot[component][model]]\n",
    "        for met, score in performance_margot[component][model].items():\n",
    "            value = \"{:.2f}\".format(score)\n",
    "            values.append(value)\n",
    "\n",
    "    cols.append(metric)\n",
    "\n",
    "# Flatten columns\n",
    "cols = sum(cols, [])\n",
    "\n",
    "# Create multi column index\n",
    "multi_columns = pd.MultiIndex.from_tuples(cols, names=['component', 'metric'])\n",
    "\n",
    "# Reshape the values\n",
    "reshaped_values = np.array(values).reshape(-1, 12)  \n",
    "\n",
    "df_margot = pd.DataFrame(reshaped_values, index=idx, columns=multi_columns)\n",
    "display(df_margot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace by a function for MARGOT and Dolly 2.0\n",
    "# Load Dolly 2.0\n",
    "performance_dolly = {}\n",
    "for component_name in os.listdir(\"dolly/results\"):\n",
    "    path_to_json = f\"dolly/results/{component_name}/json\"\n",
    "    for f in os.listdir(path_to_json):\n",
    "        path = os.path.join(path_to_json, f)\n",
    "        with open(path) as f:\n",
    "            json_results = json.load(f)\n",
    "            performance_dolly[component_name] = json_results\n",
    "\n",
    "# Convert JSON to table\n",
    "cols, values = [], []\n",
    "for component in performance_dolly:\n",
    "    tmp_col, idx = [], []\n",
    "    for model, perf in performance_dolly[component].items():\n",
    "\n",
    "        idx.append(model_labels[model])\n",
    "\n",
    "        metric = [(component, metric) for metric in performance_dolly[component][model]]\n",
    "        value = [(performance_dolly[component][model][metric]) for metric in performance_dolly[component][model]]\n",
    "        for met, score in performance_dolly[component][model].items():\n",
    "            value = \"{:.2f}\".format(score)\n",
    "            values.append(value)\n",
    "\n",
    "    cols.append(metric)\n",
    "\n",
    "# Flatten columns\n",
    "cols = sum(cols, [])\n",
    "\n",
    "# Create multi column index\n",
    "multi_columns = pd.MultiIndex.from_tuples(cols, names=['component', 'metric'])\n",
    "\n",
    "# Reshape the values\n",
    "reshaped_values = np.array(values).reshape(-1, 12)  \n",
    "\n",
    "df_dolly = pd.DataFrame(reshaped_values, index=idx, columns=multi_columns)\n",
    "display(df_dolly)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization performance with respect to text length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use baseline and argumentation dataset from earlier\n",
    "df_baseline = df_baseline['text'].fillna('')\n",
    "df_baseline['word_count'] = df_baseline['text'].str.split().apply(len)\n",
    "\n",
    "df_arg = df_margot['text'].fillna('') # MARGOT and Dolly 2.0 have same clean text, so can be used interchangeable\n",
    "df_arg['word_count'] = df_arg['text'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "# Baseline and argumentation pipeline\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(5,6))\n",
    "bins = 50\n",
    "\n",
    "sns.histplot(data=df_baseline, x='word_count', bins=bins, ax=ax[0], color='black', label=\"Baseline pipeline\")\n",
    "sns.histplot(data=df_arg, x='word_count', bins=bins, ax=ax[1], color=\"#f37651\", label=\"Argumentation-based pipeline\")\n",
    "\n",
    "ax[0].set_xticks(np.arange(0, 2000, 200))\n",
    "ax[1].set_xticks(np.arange(0, 2000, 200))\n",
    "\n",
    "ax[0].set_xlim([0, 2000])\n",
    "ax[1].set_xlim([0, 2000])\n",
    "\n",
    "ax[0].set(xlabel='Number of words', ylabel='Frequency')\n",
    "ax[1].set(xlabel='Number of words', ylabel='Frequency')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "fig.tight_layout(pad=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path_to_labels):\n",
    "    # Read test data with labels\n",
    "    df_labels = pd.read_csv(f\"{path_to_labels}.csv\", usecols=['ID', 'text', 'label'], index_col='ID').dropna()\n",
    "\n",
    "    # Convert string to int label\n",
    "    df_labels[\"label\"] = df_labels[\"label\"].map({\"FAKE\": 0, \"REAL\": 1})\n",
    "\n",
    "    # Count the number of words\n",
    "    df_labels['word_count'] = df_labels['text'].str.split().apply(len)\n",
    "\n",
    "    # Add bins depending on the words\n",
    "    df_labels['bin_id'] = pd.cut(df_labels['word_count'], bins=[0, 100, 300, 10000], labels=[\"Short\", \"Medium\", \"Long\"],right=True)\n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.combine([\"f1\"])\n",
    "\n",
    "def process_predictions(df_labels, path_to_best_model_predictions):\n",
    "\n",
    "    # Load predictions for best model\n",
    "    df_predictions = pd.read_csv(f\"{path_to_best_model_predictions}\", usecols=['ID', 'prediction'], index_col='ID')\n",
    "\n",
    "    # Combine predictions and correct labels\n",
    "    df = pd.concat([df_labels, df_predictions], axis=1)\n",
    "\n",
    "    # Group df by bin\n",
    "    grouped_df = df.groupby('bin_id')\n",
    "\n",
    "    # Calculate F1 score per bin\n",
    "    f1_per_batch = []    \n",
    "    for idx, data in grouped_df:\n",
    "        labels = data['label'].values\n",
    "        preds = data['prediction'].values\n",
    "        result = metric.compute(labels, preds)\n",
    "        print(idx, len(data), result['f1'])\n",
    "        f1_per_batch.append(result['f1'])\n",
    "    \n",
    "    return f1_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(performance, ax):\n",
    "    fig_labels = [i for i in performance]\n",
    "    y_values = [performance[i] for i in performance]\n",
    "    x_values = np.arange(1, len(y_values[0]) + 1)\n",
    "\n",
    "    for i in range(len(y_values)):\n",
    "        ax.plot(x_values, y_values[i], label=fig_labels[i])\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "baseline_path_to_labels = \"baseline/data/test\"\n",
    "base_path_best_model_preds = \"baseline/results/roberta-base_predictions.csv\"\n",
    "performance_baseline = process_predictions(get_labels(baseline_path_to_labels), base_path_best_model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_component(path_to_labels, best_models):\n",
    "    performance = {}\n",
    "    df_labels = get_labels(path_to_labels)\n",
    "\n",
    "    for comp in ['structure', 'claim', 'evidence']:\n",
    "        f1_scores = process_predictions(df_labels, best_models[comp])\n",
    "        performance[comp] = f1_scores\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MARGOT \n",
    "path_to_labels = \"models/argumentation-based/argumentation structure\" # # MARGOT and Dolly 2.0 have same clean text, so can be used interchangeable\n",
    "\n",
    "performance_margot = per_component(f\"margot/models/test\", \"distilbert-base-uncased_predictions\")\n",
    "performance_dolly = per_component(f\"/dolly/models/test\", \"distilbert-base-uncased_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the labels for graph\n",
    "performance_margot = {'Evidence': performance_margot['evidence'], 'Claim': performance_margot['claim'], 'Structure': performance_margot['structure']}\n",
    "performance_dolly = {'Evidence': performance_dolly['evidence'], 'Claim': performance_dolly['claim'], 'Structure': performance_dolly['structure']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE + MARGOT \n",
    "sns.set_style(\"white\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.lineplot(performance_baseline, marker='o',linestyle='--', color=\"black\", markersize=5, label='Baseline')\n",
    "ax.set_xticks(range(len(performance_baseline)))\n",
    "ax.set_xticklabels(['Short', 'Medium', 'Long'])\n",
    "ax.set(xlabel='Text length', ylabel='F1')\n",
    "ax.set(title=\"MARGOT\")\n",
    "\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "for i, (label, data) in enumerate(performance_margot.items()):\n",
    "    print(label)\n",
    "    sns.lineplot(x=[0, 1, 2], y=data, marker='D', label=label.capitalize(), markersize=5, ax=ax, color=palette[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE + Dolly 2.0 # TODO: Write function for both \n",
    "sns.set_style(\"white\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.lineplot(performance_baseline, marker='o',linestyle='--', color=\"black\", markersize=5, label='Baseline')\n",
    "ax.set_xticks(range(len(performance_baseline)))\n",
    "ax.set_xticklabels(['Short', 'Medium', 'Long'])\n",
    "ax.set(xlabel='Text length', ylabel='F1')\n",
    "ax.set(title=\"MARGOT\")\n",
    "\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "for i, (label, data) in enumerate(performance_margot.items()):\n",
    "    print(label)\n",
    "    sns.lineplot(x=[0, 1, 2], y=data, marker='D', label=label.capitalize(), markersize=5, ax=ax, color=palette[i])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
